# -*- coding: utf-8 -*-
"""SHE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XZLyw1a2PfpFAOwb_CftHNeZsxPHCF5
"""


"""# New Section"""

classifierweight=1

# @title
#-*- coding: utf-8 -*-
import numpy as np
import gc
import pickle
# import sys
# reload(sys)
# sys.setdefaultencoding('utf-8')



from gensim.models.fasttext import FastText

stopwords = ['rt','amp','url','sir','day','title','shri','crore','time',"a", "about","above", "across", "after", "afterwards", "again", "all", "almost", "alone", "along", "already", "also","although","always","am","among", "amongst", "amoungst", "amount",  "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "around", "as",  "at", "back","be","became", "because","become","becomes", "becoming", "been", "before", "beforehand", "behind", "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry", "de", "describe", "detail", "do", "done", "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", "front", "full", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "hundred", "ie", "if", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter", "latterly", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own","part", "per", "perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", "she", "should", "show", "side", "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "system", "take", "ten", "than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "thickv", "thin", "third", "this", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "top", "toward", "towards", "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "we", "well", "were", "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", "why", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves", "the"]


#load your pretrained semantic embedding

import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('wikidump2017/model.txt', binary=False) 
# model = FastText.load('/content/drive/MyDrive/embedding/Tweets_multiplex_network.model')
idx=1
words = []
word2idx = {}
vectors=[]
vect_idx=[]

# Indexing of word and corresponding vector
vectors.append(np.asarray([0]*model.vector_size))
with open('wikidump2017/model.txt', 'rb') as f:
    for i,l in enumerate(f):
        if i > 0:
            # print(l)
            line = l.decode().split()
            if line[1] != '-nan':
                word = line[0]
                words.append(word)
                word2idx[word] = idx
                vect = line[1:]
                vectors.append(np.asarray(vect))
                vect_idx.append(idx)
                idx += 1
                

print(len(vectors), len(word2idx), len(vectors[0]))

# input vector length
emb=len(vectors[0])

# conversion into numpy arrays
vectors=np.asarray(vectors)
vect_idx=np.array(vect_idx)

# splitting data into 80% training and 20% testing 
tss=int(len(word2idx) * 0.8)
train, test = vectors[:tss][:], vectors[tss:][:]
train_idx, test_idx = vect_idx[:tss][:], vect_idx[tss:][:]

# Sent is a dictonary with 3 keys namely positive , negative and neutral and 
# it stores the corresponding values in the form of a list
sent=dict()


#load sentiment lexicon
f=open('data.txt', 'r')
lines=f.readlines()

# Filling of the dictionary Sent
for l in lines:
    # word is a list with two elements first is word and second is polarity
    word=l.lower().strip().split('\t')
    # print(word)
    if len(word)<2:
      continue
    if word[1] not in sent:
        sent[word[1]]=[]
    sent[word[1]].append(word[0])
f.close()

# Lists of polarities initialised
positive=[]
negative=[]
neutral=[]

# Gives the list of IDs of the words in a list
def gen_index(wlist,word2idx):
    ip=[]
    for i in wlist:
        try:
            idx=word2idx[i]
            ip.append(idx)
        except:
            pass
    return ip

# Spliting of the list for K-fold Cross Validation
def split_list(alist, wanted_parts=1):
    length = len(alist)
    return [ alist[i*length // wanted_parts: (i+1)*length // wanted_parts] 
             for i in range(wanted_parts) ]

# New dictionary initialized
folds=dict()

# Store the IDs of the particular polarity in their respective lists
positive = gen_index(sent['positive'],word2idx)
negative = gen_index(sent['negative'],word2idx)
neutral = []

from random import shuffle

print("Data preparation for fold ")

# Contains the IDs of the positive polarity words.
fold=[]
for idx in sent['positive']:
    fold.append(idx)
  
# Indices Shuffled to get random sampling of the data
shuffle(fold)
folds['positive']=split_list(fold, wanted_parts=10)

# Contains the IDs of the negative polarity words.
fold=[]
for idx in sent['negative']:
    fold.append(idx)

# Indices Shuffled to get random sampling of the data
shuffle(fold)
folds['negative']=split_list(fold, wanted_parts=10)

# Contains the IDs of the neutral polarity words.
fold=[]
for idx in sent['neutral']:
    fold.append(idx)

# Indices Shuffled to get random sampling of the data
shuffle(fold)
folds['neutral']=split_list(fold, wanted_parts=10)

# Data of folds is stored in a file
f_out = open('data/3class_sent_fold.pkl','wb')
pickle.dump(folds,f_out)
f_out.close()


# f_out = open('SHE/SHE_new_fasttext_3class_sent_fold.pkl','rb')
# folds=pickle.load(f_out)
# f_out.close()


from keras.layers import Input, Dense
from keras.models import Model
from tensorflow.python.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SimpleRNN,GRU,CuDNNGRU,Reshape
from keras.layers import Conv1D, GlobalMaxPooling1D, Activation, Flatten, UpSampling1D
from keras.layers.convolutional import MaxPooling1D
from keras.utils import plot_model
import tensorflow as tf
from keras.models import model_from_json
import pickle
from sklearn.metrics import accuracy_score
from keras import backend as K

# Get the class labels of the words in the form of one hot encoding 
for fi in range(1):
    
    # Iterate over other folds to get the testing data for K-cross validation
    testing=[]
    for sample in folds['positive'][fi]:
        try:
            idx=word2idx[sample]
            testing.append([idx,[1,0,0]])
        except:
            pass
    
    for sample in folds['negative'][fi]:
        try:
            idx=word2idx[sample]
            testing.append([idx,[0,1,0]])
        except:
            pass
    
    for sample in folds['neutral'][fi]:
        try:
            idx=word2idx[sample]
            testing.append([idx,[0,0,1]])
        except:
            pass

    # Iterate over other folds to get the training data for K-cross validation
    training=[]
    for fj in range(10):
        if fj != fi:
            for sample in folds['positive'][fj]:
                try:
                    idx=word2idx[sample]
                    training.append([idx,[1,0,0]])
                except:
                    pass
            
            for sample in folds['negative'][fj]:
                try:
                    idx=word2idx[sample]
                    training.append([idx,[0,1,0]])
                except:
                    pass
            
            for sample in folds['neutral'][fj]:
                try:
                    idx=word2idx[sample]
                    training.append([idx,[0,0,1]])
                except:
                    pass
            
            #training+=folds['neutral'][fj]
   
    print( "Training fold begin",fi)

    # Training and testing labels
    training_labels=[]
    testing_labels=[]

    # Training and testing IDs
    training_idx=[]
    testing_idx=[]

    # Training and testing data for Autoencoder (AE)
    training_AE=[]
    testing_AE=[]   
    
    # Fill the traning data attributes
    for i in training:
        idx=i[0]
        training_labels.append(i[1])
        training_AE.append(vectors[idx])
        training_idx.append(idx)

    # Convert the list to numpy array
    training_AE=np.array(training_AE)
    training_labels=np.array(training_labels)
    training_idx=np.array(training_idx)

     # Fill the testing data attributes
    for i in testing:
        idx=i[0]
        testing_labels.append(i[1])
        testing_AE.append(vectors[idx])
        testing_idx.append(idx)
    
    # Convert the list to numpy array
    testing_AE=np.array(testing_AE)
    testing_labels=np.array(testing_labels)
    testing_idx=np.array(testing_idx)

    # Shape of embedded word vector
    input_node=Input(shape=(emb,))

    # Encoding layers of Autoencoder

    # Reshape
    encode=Reshape((1,emb))(input_node)

    # 1D convolution layer
    encode=Conv1D(emb,
                     3,
                     padding='same',
                     activation='relu',
                     strides=1)(encode)
    encode=MaxPooling1D(pool_size=1)(encode)

    # Dropout Set to 0.2
    decoder=Dropout(0.2)(encode)
    # decoder=tf.keras.layers.BatchNormalization()(encode)

    # Decoding layers of Autoencoder

    # 1D convolution layer
    decoder=Conv1D(64,
                     3,
                     padding='same',
                     activation='relu',
                     strides=1)(decoder)
    
    # Max Pooling layer
    decoder=GlobalMaxPooling1D()(decoder)

    # Dense layer to get output of autoencoder 
    decode=Dense(emb,activation='tanh',name="dense_1")(decoder)
    
    # 1D convolution layer for classifier ( For gettting the polarity of the sentiment)
    classifier=Conv1D(64,
                     3,
                     padding='same',
                     activation='relu',
                     strides=1)(encode)

    # Dropout set to 0.2
    classifier=Dropout(0.2)(classifier)
    # classifier=tf.keras.layers.BatchNormalization()(classifier)
    # MaxPooling layer
    classifier=GlobalMaxPooling1D()(classifier)
    # Output Layer for sentiment polarity
    # classifier=Dense(300,name='dense_3',activation='tanh')(classifier)
    senti_class=Dense(3,activation='softmax',name="dense_2")(classifier)
    Model(input_node,senti_class).summary()
    # Model for encoder
    encoder = Model(input_node, encode)
    
    # Model for autoencoder
    autoencoder=Model(input_node,decode)

    # Loss function for auto encoder chossen as MSE (mean square error)
    autoencoder.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

    # Print the summary of the model (auto encoder)
    autoencoder.summary()

    # Combine the 2 Models (Autoencoder and the Classifer)
    combine=Model(input_node,[decode,senti_class])

    # Loss functions for the 2 Models (AE and the classifer respectively)
    losses = {
        "dense_1": "mean_squared_error",
        "dense_2": "categorical_crossentropy",
    }

    # Scope of improvement by changing the weights
    lossWeights = {"dense_1": 1.0, "dense_2": classifierweight}

    # Compile the Model (combine) using the loss function losses
    combine.compile(optimizer='adam', loss=losses, loss_weights=lossWeights,
        metrics=["accuracy"])
    
    # Get the summary of the Model combine
    combine.summary()

    # Typecasting of the embeddings from string to float (Added by our group)
    test = test.astype(np.float)
    train = train.astype(np.float)
    testing_AE = testing_AE.astype(np.float)
    training_AE = training_AE.astype(np.float)
    print('XXXXXXXXXXXXXX')
    print(len(train))
    # Incremental training of our models
    # modelxgb=XGBClassifier()
    for learn in range(5):
        autoencoder.fit(train, train,
                    epochs=20,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(test, test))
                    
        combine.fit(training_AE, [training_AE, training_labels],
                    epochs=20,
                    batch_size=128,
                    shuffle=True,
                    validation_data=(testing_AE, [testing_AE, testing_labels]))
        # modelxgb.fit(training_AE,[training_AE,training_labels])
    # Store the models in some file
    model_json = combine.to_json()
    with open("model/senti_autoencoder_combine.json", "w") as json_file:
        json_file.write(model_json)
    combine.save_weights("model/senti_autoencoder_combine.h5")

    model_json = encoder.to_json()
    with open("model/3class_encoder.json", "w") as json_file:
        json_file.write(model_json)
    encoder.save_weights("model/3class_encoder.h5")
    
    
    # File to store the predicttions of the polarities of the words
    encoder.summary()
    f2=open('model/SHE_encoder_out.emb','w')
    f2.write(str(len(word2idx))+' '+str(emb)+'\n')
    for key in word2idx:
        f2.write(key.replace(' ','')+" ")
        w2v=model[key]
        result=encoder.predict(np.asarray([w2v]),verbose=0)
        for i in list(result[0][0]):
            f2.write(str(i)+" ")
        f2.write("\n")
    f2.close()

    # deletion of all the models
    del input_node
    del encode
    del classifier
    del senti_class
    del autoencoder
    del combine

    gc.collect()   

    # Clear the Keras session
    K.clear_session()

exit()